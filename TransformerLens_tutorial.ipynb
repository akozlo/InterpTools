{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to TransformerLens",
    "\n",
    "## Introduction",
    "\n",
    "[TransformerLens](https://github.com/neelnanda-io/TransformerLens) is a Python library designed to facilitate interpretability and analysis of Transformer-based language models. It provides tools to load pre-trained models, inspect their architecture, and extract activation patterns during inference.",
    "\n",
    "In this notebook, we'll cover the following tasks:",
    "\n",
    "1. **Loading a model.**",
    "2. **Extracting descriptions of the model's architecture.**",
    "3. **Extracting activation patterns during inference.**",
    "\n",
    "Let's get started!",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading a Model",
    "\n",
    "First, we'll install and import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install TransformerLens if it's not already installed",
    "!pip install transformer_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules",
    "import transformer_lens",
    "from transformer_lens import HookedTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's load a pre-trained model. For this example, we'll use the **GPT-2 Small** model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the GPT-2 Small model",
    "model_name = 'gpt2-small'",
    "model = HookedTransformer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---",
    "\n",
    "## 2. Extracting Model Architecture Details",
    "\n",
    "Now that we've loaded the model, let's extract some information about its architecture.",
    "\n",
    "### Number of Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total number of parameters",
    "total_params = sum(p.numel() for p in model.parameters())",
    "print(f\"Total number of parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of layers",
    "num_layers = model.cfg.n_layers",
    "print(f\"Number of layers: {num_layers}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality of Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the embedding dimension",
    "d_model = model.cfg.d_model",
    "print(f\"Embedding dimension (d_model): {d_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the vocabulary size",
    "vocab_size = model.cfg.d_vocab",
    "print(f\"Vocabulary size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heads per Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of attention heads",
    "n_heads = model.cfg.n_heads",
    "print(f\"Number of attention heads: {n_heads}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---",
    "\n",
    "## 3. Extracting Activation Patterns for a Given Inference",
    "\n",
    "To extract activation patterns, we'll perform an inference on a sample input and use the model's caching mechanisms.",
    "\n",
    "### Sample Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a sample input",
    "input_text = \"Hello, how are you?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the input text",
    "tokens = model.to_tokens(input_text)",
    "print(f\"Token IDs: {tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Inference with Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model and collect activations using the cache",
    "output, cache = model.run_with_cache(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting Activations",
    "\n",
    "The cache contains activations from various points in the model. You can access them using keys that correspond to specific components and layers.",
    "\n",
    "#### Residual Stream Before First Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resid_pre_0 = cache['resid_pre', 0]",
    "print(f\"Shape of residual stream before layer 0: {resid_pre_0.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attention Output at First Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_out_0 = cache['attn_out', 0]",
    "print(f\"Shape of attention output at layer 0: {attn_out_0.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLP Output at First Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_out_0 = cache['mlp_out', 0]",
    "print(f\"Shape of MLP output at layer 0: {mlp_out_0.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Residual Stream After First Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resid_post_0 = cache['resid_post', 0]",
    "print(f\"Shape of residual stream after layer 0: {resid_post_0.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Attention Patterns",
    "\n",
    "You can also visualize the attention patterns of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the attention patterns from layer 0",
    "attn_patterns = cache['pattern', 0]  # Shape: (batch, heads, query_pos, key_pos)",
    "print(f\"Shape of attention patterns at layer 0: {attn_patterns.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---",
    "\n",
    "## Conclusion",
    "\n",
    "In this notebook, we've:",
    "\n",
    "- **Loaded a pre-trained Transformer model using TransformerLens.**",
    "- **Extracted key architectural details of the model.**",
    "- **Captured and inspected activation patterns during inference.**",
    "\n",
    "TransformerLens provides a powerful interface for deep diving into Transformer models, enabling researchers and practitioners to better understand how these models process and generate language.",
    "\n",
    "---",
    "\n",
    "## Additional Resources",
    "\n",
    "- [TransformerLens GitHub Repository](https://github.com/neelnanda-io/TransformerLens)",
    "- [TransformerLens Documentation](https://neelnanda-io.github.io/TransformerLens/)",
    "\n",
    "Feel free to explore further by inspecting activations from other layers or components of the model!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
